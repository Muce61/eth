#!/usr/bin/env python3
"""
ä»Binance Data Portalä¸‹è½½æ‰€æœ‰762ä¸ªåˆçº¦çš„1ç§’Kçº¿æ•°æ®
æ—¶é—´èŒƒå›´: 2024-12-01 åˆ° 2025-12-01
"""
import requests
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import time
import zipfile
from concurrent.futures import ThreadPoolExecutor

# ä»leverage_brackets.csvåŠ è½½æ‰€æœ‰å¸ç§
print("åŠ è½½å¸ç§åˆ—è¡¨...")
leverage_df = pd.read_csv('/Users/muce/PycharmProjects/github/eth/leverage_brackets.csv')
ALL_SYMBOLS = leverage_df['symbol'].tolist()
print(f"æ€»è®¡: {len(ALL_SYMBOLS)}ä¸ªå¸ç§\n")

# Binance Data Portal URL
BASE_URL = "https://data.binance.vision/data/futures/um/daily/aggTrades"

def download_aggtrades(symbol, date_str, output_dir):
    """ä¸‹è½½å•æ—¥aggTradesæ•°æ®"""
    url = f"{BASE_URL}/{symbol}/{symbol}-aggTrades-{date_str}.zip"
    
    try:
        response = requests.get(url, timeout=30)
        
        if response.status_code == 200:
            # æ£€æŸ¥æ˜¯å¦æ˜¯XMLé”™è¯¯
            if response.content.startswith(b'<?xml') or response.content.startswith(b'<Error>'):
                return None
                
            # ä¿å­˜ZIPæ–‡ä»¶
            zip_path = output_dir / f"{symbol}-{date_str}.zip"
            with open(zip_path, 'wb') as f:
                f.write(response.content)
            
            return zip_path
        else:
            return None
            
    except Exception as e:
        return None

def extract_and_convert_to_1s(zip_path, output_dir):
    """è§£å‹ZIPå¹¶å°†aggTradesè½¬æ¢ä¸º1ç§’Kçº¿"""
    try:
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆZIP
        if not zipfile.is_zipfile(zip_path):
            zip_path.unlink()
            return 0
            
        # è§£å‹ZIP
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            csv_name = zip_ref.namelist()[0]
            
            # è¯»å–CSV
            with zip_ref.open(csv_name) as csv_file:
                df = pd.read_csv(csv_file)
        
        # è½¬æ¢æ—¶é—´æˆ³ä¸ºdatetime (åˆ—åæ˜¯ transact_time)
        df['timestamp'] = pd.to_datetime(df['transact_time'], unit='ms')
        df['second'] = df['timestamp'].dt.floor('1s')
        
        # æŒ‰ç§’èšåˆ
        klines_1s = df.groupby('second').agg({
            'price': ['first', 'max', 'min', 'last'],
            'quantity': 'sum'
        }).reset_index()
        
        klines_1s.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        
        # ä¿å­˜1ç§’Kçº¿
        parts = zip_path.stem.split('-')
        if len(parts) >= 4:
            symbol = parts[0]
            date_str = f"{parts[1]}-{parts[2]}-{parts[3]}"
        else:
            symbol = zip_path.stem.split('-')[0]
            date_str = zip_path.stem.replace(f"{symbol}-", "")
            
        output_file = output_dir / f"{symbol}-{date_str}.csv"
        
        klines_1s.to_csv(output_file, index=False)
        
        # åˆ é™¤ZIPæ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
        zip_path.unlink()
        
        return len(klines_1s)
        
    except Exception as e:
        return 0

def process_single_date(args):
    """å¤„ç†å•ä¸ªæ—¥æœŸçš„ä»»åŠ¡ (ç”¨äºå¤šçº¿ç¨‹)"""
    symbol, date, raw_dir, processed_dir = args
    date_str = date.strftime('%Y-%m-%d')
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    output_file = processed_dir / f"{symbol}-{date_str}.csv"
    if output_file.exists():
        return 1  # å·²å­˜åœ¨
    
    # ä¸‹è½½
    zip_path = download_aggtrades(symbol, date_str, raw_dir)
    
    if zip_path:
        # è½¬æ¢
        rows = extract_and_convert_to_1s(zip_path, processed_dir)
        return rows if rows > 0 else 0
    return 0

def merge_daily_to_monthly(symbol, year, month, processed_dir, monthly_dir):
    """åˆå¹¶å•æ—¥æ–‡ä»¶ä¸ºæœˆåº¦æ–‡ä»¶"""
    pattern = f"{symbol}-{year}-{month:02d}-*.csv"
    daily_files = sorted(processed_dir.glob(pattern))
    
    if not daily_files:
        return False
    
    print(f"  åˆå¹¶ {symbol} {year}-{month:02d}: {len(daily_files)}å¤© ... ", end='', flush=True)
    
    try:
        dfs = []
        for f in daily_files:
            df = pd.read_csv(f)
            dfs.append(df)
        
        merged = pd.concat(dfs, ignore_index=True)
        merged['timestamp'] = pd.to_datetime(merged['timestamp'])
        merged = merged.sort_values('timestamp').drop_duplicates()
        
        # ä¿å­˜æœˆåº¦æ–‡ä»¶
        output_file = monthly_dir / f"{symbol}-{year}-{month:02d}.csv"
        merged.to_csv(output_file, index=False)
        
        # åˆ é™¤æ—¥åº¦æ–‡ä»¶
        for f in daily_files:
            f.unlink()
        
        file_size_mb = output_file.stat().st_size // 1024 // 1024
        print(f"âœ“ {len(merged):,}è¡Œ, {file_size_mb}MB")
        return True
        
    except Exception as e:
        print(f"âœ— {e}")
        return False

def main():
    print("="*70)
    print("Binance å…¨å¸‚åœº1ç§’Kçº¿æ•°æ®ä¸‹è½½å™¨ (762ä¸ªå¸ç§)")
    print("="*70)
    print(f"æ—¶é—´èŒƒå›´: 2024-12-01 åˆ° 2025-12-01")
    print(f"å¸ç§æ•°é‡: {len(ALL_SYMBOLS)}")
    print(f"é¢„è®¡ä¸‹è½½: ~100-150GB")
    print(f"é¢„è®¡æ—¶é—´: 15-20å¤© (å¤šçº¿ç¨‹)")
    print("="*70)
    
    # åˆ›å»ºç›®å½•
    raw_dir = Path("/Users/muce/1m_data/1s_data/raw")
    processed_dir = Path("/Users/muce/1m_data/1s_data/processed")
    monthly_dir = Path("/Users/muce/1m_data/1s_data/monthly")
    
    raw_dir.mkdir(parents=True, exist_ok=True)
    processed_dir.mkdir(parents=True, exist_ok=True)
    monthly_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
    start_date = datetime(2024, 12, 1)
    end_date = datetime(2025, 12, 1)
    
    date_list = []
    current = start_date
    while current <= end_date:
        date_list.append(current)
        current += timedelta(days=1)
    
    print(f"\næ€»å¤©æ•°: {len(date_list)}å¤©")
    print(f"æ€»ä¸‹è½½ä»»åŠ¡: {len(ALL_SYMBOLS)} Ã— {len(date_list)} = {len(ALL_SYMBOLS) * len(date_list):,}ä¸ªæ–‡ä»¶")
    print("\nå¼€å§‹ä¸‹è½½ (10çº¿ç¨‹å¹¶è¡Œ)...\n")
    
    # ç»Ÿè®¡
    total_downloaded = 0
    total_failed = 0
    total_coins = len(ALL_SYMBOLS)
    
    # æŒ‰å¸ç§ä¸‹è½½
    for i, symbol in enumerate(ALL_SYMBOLS, 1):
        print(f"[{i}/{total_coins}] {symbol} æ­£åœ¨ä¸‹è½½...")
        
        # å‡†å¤‡ä»»åŠ¡
        tasks = [(symbol, date, raw_dir, processed_dir) for date in date_list]
        
        # å¤šçº¿ç¨‹æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(process_single_date, tasks))
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r > 0)
        fail_count = len(results) - success_count
        
        total_downloaded += success_count
        total_failed += fail_count
        
        print(f"  {symbol} å®Œæˆ: âœ“{success_count} âœ—{fail_count}")
        
        # åˆå¹¶æœˆåº¦æ–‡ä»¶
        if success_count > 0:
            print(f"  åˆå¹¶æœˆåº¦æ–‡ä»¶...")
            for year in [2024, 2025]:
                for month in range(1, 13):
                    merge_daily_to_monthly(symbol, year, month, processed_dir, monthly_dir)
        
        # æ¯10ä¸ªå¸ç§æ‰“å°ä¸€æ¬¡æ€»è¿›åº¦
        if i % 10 == 0:
            progress_pct = (i / total_coins) * 100
            print(f"\nğŸ“Š æ€»è¿›åº¦: {i}/{total_coins} ({progress_pct:.1f}%)")
            print(f"   æˆåŠŸ: {total_downloaded:,} | å¤±è´¥: {total_failed:,}\n")
    
    print("\n" + "="*70)
    print("ä¸‹è½½å®Œæˆæ‘˜è¦")
    print("="*70)
    print(f"å¸ç§æ€»æ•°: {total_coins}")
    print(f"æˆåŠŸ: {total_downloaded:,}ä¸ªæ–‡ä»¶")
    print(f"å¤±è´¥: {total_failed:,}ä¸ªæ–‡ä»¶")
    
    # æ£€æŸ¥ç£ç›˜ä½¿ç”¨
    import subprocess
    result = subprocess.run(['du', '-sh', str(monthly_dir)], capture_output=True, text=True)
    print(f"ç£ç›˜ä½¿ç”¨: {result.stdout.strip()}")
    
    print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜è‡³: {monthly_dir}")

if __name__ == "__main__":
    main()
